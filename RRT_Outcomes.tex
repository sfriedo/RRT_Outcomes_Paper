
%% bare_conf_compsoc.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE Computer
%% Society conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************
\documentclass[conference,compsoc]{IEEEtran}

% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi

\usepackage{textcomp}
\usepackage{graphicx}
\graphicspath{ {figures/} }


\begin{document}
\title{Prediction of Patient Outcomes after \\Renal Replacement Therapy (RRT) in the ICU }


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Harry Freitas Da Cruz}
\IEEEauthorblockA{Hasso Plattner Institute (HPI) \\
Enterprise Platform and Integration Concepts \\
Potsdam, Germany\\
Email: Harry.FreitasDaCruz@hpi.de}
\and
\IEEEauthorblockN{Siegfried Horschig}
\IEEEauthorblockA{Hasso Plattner Institute (HPI) \\
Enterprise Platform and Integration Concepts \\
Potsdam, Germany\\
Email: siegfried.horschig@student.hpi.de}
}
\maketitle

\begin{abstract}
In order to compensate impairments of the renal system in the human body, artificial methods in the form of renal replacement therapy (RRT), called dialysis, have to be introduced.
Many parameters of the dialysis can be adjusted and the outcome of the procedure may change with different patient characteristics. 
In this paper, we introduce a clinical decision support system to predict the effect of a given dialysis on a patient while in the intensive care unit (ICU). \\
For this purpose, we employ two kinds of machine learning models: Bayesian Rule Lists (BRL) and Deep Neural Networks (DNN). Although the DNN may provide better accuracy, its decision making is not easily interpretable for humans. For this reason, we use mimic learning as a method to make the DNN interpretable. \\
Results show us that the DNN outperforms our BRL classifier as expected, but by a rather small margin.
For the mimic learning process, we used a bayesian ridge regression model.
Even though the regression model performs worse when training as a mimic model as opposed to drectly on the data, it provides some insight into the inner workings of the DNN.
\end{abstract}


\section{Introduction}
The renal system in the human body has the purpose to eliminate wastes from the body and control levels of certain substances in the blood. 
If this system is impaired, for example due to Acute Kidney Injury (AKI), artificial methods in the form of Renal Replacement Therapy (RRT) have to be introduced, more commonly known under the term of dialysis. \\
There are different options for dialysis available.
One example is the hemodialysis, where the patient's blood is pumped through a dialyzer, inside of which is a liquid called dialysate.
This liquid's composition determines which substances should be filtered out of the blood.
The dialyzer separates the blood and the dialysate though a partially permeable membrane, allowing for the filtering of the blood through osmosis.
Another example for the dialysis is the peritoneal dialysis, which uses the peritoneal cavity inside the patient as a container for the dialysate. \\

The dialysis’ outcomes are highly dependent on both the patient’s characteristics and the parameters as well as the type of the dialysis. 
So, usually, patients undergoing the peritoneal dialysis experience lesser health issues related to the dialysis than those undergoing hemodialysis, as there is less pressure on the circulatory system.
On the other hand, the hemodialysis is more efficient in such a way that it needs less time for the same amount of filtration. \\
Especially the hemodialysis is a costly process which needs specialized equipment and therefore has many parameters to be tuned. 
These include, but are not limited to the duration of the process, the filtration rate and flow rates of the blood and dialysate.
We want to introduce a machine learning model that, based on those parameters as input, allows to make a prediction for the health of the patient after the procedure. \\

Aside from usual criteria like accuracy or recall, when employing a machine learning model in the medical context one especially important factor is the interpretability of the model.
This is due to the fact that the doctors want to make decisions based on those predictions and take full responsibility for the consequences, so they have to validate the decision making process.
Oftentimes, models just show correlations between various parameters, and those correlations have to be manually checked for causal relations.
Additionally, the European Union introduced regulations (taking effect 2018) that give consumers in any sector a "right to explanation".
Essentially, this means that in any decision making process that is done by machines, be it a credit application or a diagnosis, the individual has the right to access meaningful information about the logic involved. \\

This way, we can roughly separate machine learning algorithms in two categories: interpretable and non-interpretable. 
One example for interpretable models are Bayesian Rule Lists (BRL).
By presenting itself as \emph{if...then...else} lists, it is easy for humans to comprehend both the decision making and the individual influence of each parameter on the outcome. \\
Deep Neural Networks (DNN) on the other hand are non-interpretable, because the weights of the nodes in the hidden layers is everything they expose to the outside.
Due to the fact that different loss and activation functions take effect when updating those weights, the abstraction to the original input data is just too large for a human to grasp. \\
For this reason, we want to make non-interpretable models interpretable, giving some insight into their inner workings and decision making. \\

The main purpose of this paper is to develop a clinical decision support system to determine patient-specific outcomes after RRT in the ICU and evaluate the interpretability of different models used in this process. 
We employ two different models, BRL as the interpretable one and DNN as the non-interpretable one.
Afterwards, we show performance comparison of both those models and try to give some insight into the decision-making process of the DNN using mimic learning.


\section{Related Work}
In the field of prediction concerning the renal system, a lot of work has already been done.
In \cite{Schwenger2012} mortality of patients after RRT in the ICU was observed and set in relation to the method of dialysis, proposing a new cost-efficient using a single-pass batch system. The dosage of dialysis was discussed and set in correlation with blood values in \cite{Ricci2006}. \\
Oftentimes, mortality is used as a target outcome in the renal context \cite{Barrett1997} \cite{Lakshmi2014} \cite{Kusiak2005}.
When it comes to model selection, \cite{Baby2015} found a bayes algorithm suitable, while \cite{Lakshmi2014} compared regression, random forest and artificial neural networks and proposed the latter for better performance and accuracy. Other models include decision trees \cite{Greco2010} and support vector machines \cite{Vijayarani2015} \cite{Sinha2015}. \\
The term \emph{interpretability} in the context of machine learning is defined and its extent discussed in \cite{Lipton2016}. In \cite{Katuwal2016} methods of getting interpretability into models in the medical context is researched. Especially for the prediction in the ICU, \cite{Che2016} proposes mimic learning as a technique to be used for interpretability. \\

In the context of this work, we try to unify both the prediction of patient outcomes in the renal context as well as the interpretability of the models used.
We compare the results of mimic learning as described in \cite{Che2016} with the results of an interpretable classifier, Bayesian Rule Lists as proposed in \cite{Letham2015}.


\section{Tools}
For quick prototyping, we used \emph{RapidMiner} \cite{RapidMiner}, allowing us to prepare data, develop and cross-validate first models. 
For deeper model development, we implemented them with the \emph{scikit-learn} library \cite{SKLearn} in Python. \\
The Data we used was provided by the \emph{MIMICIII} dataset \cite{Johnson2016} stored in a \emph{HANA} database \cite{Farber2012}.


\section{Data}
The \emph{MIMICIII} dataset contains hospital admission data for patient collected over an eleven-year period in a Boston hospital. 
As seen in figure \ref{fig:cohort}, out of the approximately 46,000 patients present in the dataset, we extracted 925 relevant patients for us, totaling to approximately 3,000 dialysis procedures we can train our models upon.

\begin{figure}[h]
	\includegraphics[width=0.5\textwidth]{cohort.jpg}
	\label{fig:cohort}
	\caption{Cohort selection of the dialysis procedures based on the patients.}
\end{figure}

For each procedure, we have about 80 features defining it. 
Those include patient demographics such as age or BMI, dialysis parameters such as the duration of the procedure, comorbidities as well as lab values. 
Additionally, we include patient vitals and measurements on how "well" the patient is doing such as 90-day mortality, the number of days he/she spent without mechanical ventilation and in the ICU in general.

\subsection{Missing Data}
Due to the manually curated nature of the \emph{MIMICIII} dataset, aside from the inconsistencies within, a lot of data is missing. 
\emph{TODO: Retrieve numbers!!!}
As the scikit-learn models need a complete dataset for training, we decided to impute the missing values. 
We evaluated both mean/median imputation and K-Nearest-Neighbour imputation and decided on the latter, giving us an increase of about 2\% in accuracy.


\section{Models}
In order to compare performance of both interpretable and non-interpretable models, we trained one model for each category. 
In the following section, we describe the models and strategies used as well as the parameters chosen for training.

\subsection{Interpretable - Bayesian Rule Lists}
For the interpretable model, we chose the existing Python 2 implementation of Bayesian Rule Lists (BRL) as described in \cite{Letham2015}. 
They pose itself as a direct competitor to decision tree approaches, as they have a high accuracy for classification while still being easy to read for humans. \\
This algorithm tries to find \emph{if...then...else} statements over a dataset with the important criteria of them being sparse for better human readability. 
It achieves this goal by mining antecendents from the data and afterwards computing the posterior distribution over the antecedent lists.\\
The current implementation of BRL has the shortcoming of only being able to classify binary targets. 
Thus, we have to adjust the target features accordingly.
\subsubsection{Parameters}
The sole adjustable parameter in the used implementation is the maximum number of iterations. 
Multiple adjustments to this parameter - including changes by factor 10 - did not result in a significant change, neither for the runtime nor for the accuracy. 
For the evaluation, we chose a value of 50,000 maximum iterations.

\subsection{Non-Interpretable - Deep Neural Network}
As non-interpretable model, we chose the powerful and widely used Deep Neural Network (DNN). 
Specifically, the scikit-learn implementation Multi-Layer Perceptron (MLP). 
Just as other implementations, this network consists of multiple layers of so-called "neurons": one input layer with as many neurons as there are inputs, one output layer with the size of the number of target features and hidden layers varying in size and quantity. 
The weights of each neuron is updated after each iteration of training, optimizing the log-loss function. \\
Scikit-learn offers implementations for both regression and classification tasks.
\subsubsection{Parameters}
Neural networks have a wealth of parameters to be adjusted. 
Doing a grid search over some of the parameters, we found the default ones from the library to perform the best.\\
This means the learning rate - determining the speed and accuracy of convergence - is set to 0.001. 
The activation function, determining the output of the neurons in the hidden layer, is the rectifier linear unit "relu". 
The network consists of one hidden layer with 100 neurons. 
The maximum number of iterations before convergence is set to 200.

\subsection{Interpretablity Approach - Mimic Learning}
The large amount of neurons in the DNN and the many parameters influencing their weights and output make it very difficult - if not impossible - for a human to understand the influence of each feature on the training. \\
That is why we wanted to give some insight into the workings of the DNN by applying a method called \emph{mimic learning}. 
Orientated on the works of \cite{Che2016}, we train an interpretable model - the so-called "mimic model" - on the outputs of the non-interpretable model.
The mimic model takes the same input features as the non-interpretable model.
For classification tasks, the output of the non-interpretable model are called "soft scores", because as they are probabilites, they are continuous variables, coming close to the actual target features.
In theory, the training of the mimic model on the soft scores allows to create a much smaller, thus understandable, faster but still equally accurate model.
Using the principle of knowledge distillation, it is even possible for the mimic model to perform better than the non-interpretable model. \\
In our case, the DNN is the non-interpretable model.
For the mimic model, we need a model, which is able to predict continuous scores.
We decided on Bayesian Ridge Regression.
\subsubsection{Bayesian Ridge Regression}
The Bayesian Ridge Regression, like common linear regression, tries to find coefficients for each input feature so that they map to the target feature, minimizing loss.
In addition to common linear regression, it includes regularization parameters to control the growth of the coefficients.
Therefore, this model is less prone to overfitting while still being as fast as linear regression.


\section{Performance Metrics}
In order to compare the models' performance to each other, we have to specify performance metrics, in this case for binary classification.
All of them are working with the terms of true and false positives and negatives.
The terms \emph{positives} and \emph{negatives} refer to the prediction of the model, while the terms of \emph{true} and \emph{false} refer to the fact if the prediction of the model was correct.
Additionally, all targets that are true are \emph{relevant} elements. \\
\textbf{Sensitivity / Recall} specifies the number of relevant items that have been selected.
This means the number of true positives divided by the number of all true targets. \\
\textbf{Precision} specifies the number of relevant instances in the result set.
This means, out of all as positively classified targets, how many have actually been positive. \\
\textbf{Specificity} specifies the number of correctly classified negative instances. \\
The \textbf{Diagnostic Odds Ratio} (DOR) is defined as the ratio of the odds of the prediction being positive if the target is positive against the odds that the prediction is positive if the target is negative.
It range from zero to infinity, with a DOR of 1 meaning that the prediction is equally likely to give a positive prediction no matter the true status.
A higher value indicates a better prediction. \\
The \textbf{Receiver Operating Characteristics curve} (ROC) plots the true positive rate against the false positive rate at various thresholds.
The important measure for this plot is the \textbf{Area Under the Curve} (AUC), a value ranging from zero to one, with an AUC of 0.5 describing a random classifier.
Higher values indicate better classification performance.


\section{Results}
In the following section, we want to compare the performance of our interpretable model, the BRL, and our non-interpretable model, the DNN.
Although there are continuous values for our target variables in the dataset, we had to transform them into a binary format in order for the BRL classifier to work.
This is why the target variable for this sample is mortality within 90 days of the procedure.

\begin{figure}[h]
	\includegraphics[width=0.5\textwidth]{class_DIED_90DAYS.jpg}
	\label{fig:mortalityCLASS}
	\caption{Performance comparison of the classifiers when predicting 90-day mortality.}
\end{figure}

Figure \ref{fig:mortalityCLASS} shows general performance of the classifiers.
We can see that, as expected, the DNN outperforms the BRL classifier in every performance criteria.
The difference is about ten percent for every performance measure.

\begin{figure}[h]
	\includegraphics[width=0.5\textwidth]{brl_DIED_90DAYS.jpg}
	\label{fig:mortalityBRL}
	\caption{Excerpt of the rules from the BRL classifier when prediction 90-day mortality.}
\end{figure}

Due to the interpretable nature of the BRL, we can look at the importance of single features.
Figure \ref{fig:mortalityBRL} shows the influence of some features and their values on the prediction of 90-day mortality.
For example, we can see that a lower \emph{Elixhauser-van Walraven} score - a score combining several comorbidities - along with a lower \emph{Glomerular Filtration Rate} two days before the procedure leads to a lower possibility of death within 90 days.

\begin{figure}[h]
	\includegraphics[width=0.5\textwidth]{ROC_DIED_90DAYS.png}
	\label{fig:mortalityROC}
	\caption{ROC curve comparing the classifiers for prediction of 90-day mortality.}
\end{figure}

If we want to see how the DNN actually makes its decisions, we have to apply the mimic learning strategy.
First, we have to evaluate if the performance of the mimic model is reasonably good when only being trained on the outputs of the DNN.
Figure \ref{fig:mortalityROC} presents the performance of the mimic model - our Bayesian Ridge Regression - relative to the other classifiers.
We can see that, while the regression is still worse than the DNN, it performs better than the BRL, if only by a small margin.

\begin{figure}[h]
	\includegraphics[width=0.5\textwidth]{coef_DIED_90DAYS.png}
	\label{fig:mortalityCOEF}
	\caption{Coefficients of the most important features for the Bayesian Ridge Regression trained as mimic model when predicting for a true outcome of 90-day mortality.}
\end{figure}

If we now look into the most important coefficients of the regression in figure \ref{fig:mortalityCOEF}, we can see the influence of single features on a positive prediction of 90-day mortality.
So, for example, the higher the rightmost feature, the age of the patient, the higher is the probability of the patient to die within 90 days.
On the other hand side, the higher the leftmost feature, the potassium value in the blood of the patient, the less likely the patient is to die within 90 days.

\section{Discussion}
The classification results of the models can be improved.
While providing a sufficiently good performance for researching the interpretability, their confidence has to be higher for the practical use in the medical context. \\
Looking at the influence of single features, the results are mixed.
While some of them make sense intuitively, like a higher age correlating with a higher chance of mortality, some just appear to be coincidental correlations.
For example, there is most probably not a linear correlation between certain blood values and the mortality, as either too low or too large values can be influencing the patient's health negatively.
In the case of potassium, values between 3.5 and 5.5 \textmu g/l would be fine values, but higher or lower values can lead to heart arrhythmia. \\
The same observation holds true for the output of the BRL.
While obesity and a longer stay in the hospital are most likely indeed factors for a higher mortality, other associations with the lactate value in the blood may be random coincidence.
Especially because higher lactate values usually lead to other complications, so the upper bound of "infinity" is very rough. \\
In order to refine and validate those assumptions, it is necessary to go further with the data analysis.
Finding actual upper and lower bounds in the dataset can provide some insight to the actual values the model considers when making predictions.

By training the regression as a mimic model, we can make assumptions on how the DNN \emph{may} make its decisions.
There still is a gap between the performance of the regression model and the DNN, which makes it difficult to say how close those coefficients are to the actual influence of features in the DNN.
The mimic model performs worse when being trained on the outputs of the DNN as opposed to being trained on the real targets, because it most probably also learns the errors of the DNN.
This can be a resolvable issue by improving the performance of the DNN through further parameter tuning and data preparation.


\section{Conclusion}
In this paper, we compared the performance of different models when being used in the prediction in the renal context.
An important part is the interpretability of said models to validate the decision making.
We used a mimic learning approach to make a Deep Neural Network interpretable and compared this output to that of the interpretable model, the Bayesian Rule Lists.
Preliminary results for prediction of 90-day mortality enable the exploration of interpretability, showing the influence of single features.
While some of the features' influence can intuitively be classified as correct, others' influence needs further data examination to prove or disprove. \\

Future work includes more elaborate use of the data. 
This means the inclusion of more features, more elaborate imputation and collection of information about the patients. \\
Additionally, more target features can be added to the existing experiments. 
The binary classification limitation of the current implementation of BRL can be overcome by using a more advanced algorithm proposed by Yang \cite{Yang2016}.
When a higher precision is achieved, interpretable models can be used as a Clinical Decision Support System, allowing the doctors to validate the decisions and giving patients insight into their treatment.


\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}


